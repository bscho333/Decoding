Running chair_eval_llava.py
log_path: /home/bscho333/Decoding/output/logs/chair
import 1/3

A module that was compiled using NumPy 1.x cannot be run in
NumPy 2.0.2 as it may crash. To support both 1.x and 2.x
versions of NumPy, modules must be compiled with NumPy 2.0.
Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.

If you are a user of the module, the easiest solution will be to
downgrade to 'numpy<2' or try to upgrade the affected module.
We expect that some modules will need time to support NumPy 2.

Traceback (most recent call last):  File "/home/bscho333/Decoding/eval_bench/chair_eval_llava.py", line 19, in <module>
    from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN
  File "/home/bscho333/Decoding/experiments/llava/__init__.py", line 1, in <module>
    from .model import LlavaLlamaForCausalLM
  File "/home/bscho333/Decoding/experiments/llava/model/__init__.py", line 1, in <module>
    from .language_model.llava_llama import LlavaLlamaForCausalLM, LlavaConfig
  File "/home/bscho333/Decoding/experiments/llava/model/language_model/llava_llama.py", line 23, in <module>
    from transformers import AutoConfig, AutoModelForCausalLM, \
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1090, in __getattr__
    value = getattr(module, name)
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1089, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1099, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 32, in <module>
    from ...modeling_utils import PreTrainedModel
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/transformers/modeling_utils.py", line 86, in <module>
    from accelerate import dispatch_model, infer_auto_device_map, init_empty_weights
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/accelerate/__init__.py", line 3, in <module>
    from .accelerator import Accelerator
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/accelerate/accelerator.py", line 35, in <module>
    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/accelerate/checkpointing.py", line 24, in <module>
    from .utils import (
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/accelerate/utils/__init__.py", line 131, in <module>
    from .bnb import has_4bit_bnb_layers, load_and_quantize_model
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/accelerate/utils/bnb.py", line 42, in <module>
    import bitsandbytes as bnb
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/__init__.py", line 6, in <module>
    from . import cuda_setup, utils, research
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/research/__init__.py", line 1, in <module>
    from . import nn
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/research/nn/__init__.py", line 1, in <module>
    from .modules import LinearFP8Mixed, LinearFP8Global
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/research/nn/modules.py", line 8, in <module>
    from bitsandbytes.optim import GlobalOptimManager
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/optim/__init__.py", line 8, in <module>
    from .adagrad import Adagrad, Adagrad8bit, Adagrad32bit
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/optim/adagrad.py", line 5, in <module>
    from bitsandbytes.optim.optimizer import Optimizer1State
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/optim/optimizer.py", line 12, in <module>
    import bitsandbytes.functional as F
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/functional.py", line 169, in <module>
    def get_paged(*shape, dtype=torch.float32, device=torch.device('cuda', index=0)):
/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/bitsandbytes/functional.py:169: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)
  def get_paged(*shape, dtype=torch.float32, device=torch.device('cuda', index=0)):
import 2/3
/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().
  warnings.warn(_BETA_TRANSFORMS_WARNING)
import 3/3
first line of chair_eval_llava.py
args:  Namespace(model_path='/home/bscho333/data/llava-v1.5-7b', model_base='llava', conv_mode='llava_v1', temperature=1.0, top_p=1, top_k=None, data_path='/home/bscho333/data/coco/val2014/', anno_path='/home/bscho333/data/coco/annotations/instances_val2014.json', log_path='/home/bscho333/Decoding/output/logs/chair', out_path='/home/bscho333/Decoding/output/chair_results', seed=42, batch_size=1, num_workers=2, use_ritual=False, use_vcd=False, noise_step=500, use_m3id=False, ritual_alpha_pos=3.0, ritual_alpha_neg=1.0, ritual_beta=0.1, num_eval_samples=500, max_new_tokens=64, experiment_index=0)
args.log_path:  /home/bscho333/Decoding/output/logs/chair
Starting rank=0, seed=42, world_size=1, device=0
[2024-11-05 23:32:05] Experiment directory created at /home/bscho333/Decoding/output/logs/chair/llava-v1.5-7b/0
[2024-11-05 23:32:05] Initializing Model
Initializing Model
model_name: llava-v1.5-7b
model_path: /home/bscho333/data/llava-v1.5-7b
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.91s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.83s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]
Model loaded
model.config: LlavaConfig {
  "_name_or_path": "/home/bscho333/data/llava-v1.5-7b",
  "architectures": [
    "LlavaLlamaForCausalLM"
  ],
  "bos_token_id": 1,
  "eos_token_id": 2,
  "freeze_mm_mlp_adapter": false,
  "freeze_mm_vision_resampler": false,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "image_aspect_ratio": "pad",
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_length": 4096,
  "max_position_embeddings": 4096,
  "mm_hidden_size": 1024,
  "mm_projector_type": "mlp2x_gelu",
  "mm_resampler_type": null,
  "mm_use_im_patch_token": false,
  "mm_use_im_start_end": false,
  "mm_vision_select_feature": "patch",
  "mm_vision_select_layer": -2,
  "mm_vision_tower": "openai/clip-vit-large-patch14-336",
  "model_type": "llava",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 32,
  "pad_token_id": 0,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "tie_word_embeddings": false,
  "torch_dtype": "float16",
  "transformers_version": "4.31.0",
  "tune_mm_mlp_adapter": false,
  "tune_mm_vision_resampler": false,
  "unfreeze_mm_vision_tower": false,
  "use_cache": true,
  "use_mm_proj": true,
  "vocab_size": 32000
}

Traceback (most recent call last):
  File "/home/bscho333/Decoding/eval_bench/chair_eval_llava.py", line 305, in <module>
    main()
  File "/home/bscho333/Decoding/eval_bench/chair_eval_llava.py", line 137, in main
    print(f"model.config.image_size: {model.config.image_size}")
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/transformers/configuration_utils.py", line 261, in __getattribute__
    return super().__getattribute__(key)
AttributeError: 'LlavaConfig' object has no attribute 'image_size'
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 1914091) of binary: /home/bscho333/anaconda3/envs/RITUAL/bin/python
Traceback (most recent call last):
  File "/home/bscho333/anaconda3/envs/RITUAL/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torch/distributed/run.py", line 794, in main
    run(args)
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bscho333/anaconda3/envs/RITUAL/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
eval_bench/chair_eval_llava.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-11-05_23:32:21
  host      : node01.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 1914091)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
